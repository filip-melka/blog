---
banner: ./banners/building-vector-db.gif
description: I’ve been curious about how vector databases actually work, so I tried building a very simple one myself. Along the way, I learned the basics of embeddings and similarity search, and this article is my attempt to share what I’ve figured out so far.
slug: building-vector-db
tags:
  - vector-db
  - embedding
  - ai
title: So… What Even Is a Vector Database?
---

import { Query } from '@blog/ui-library'

Every time you search in Google Photos for ‘dog’, or ask ChatGPT a question, under the hood you’re using a **vector database**. It’s one of those technologies that suddenly seems to be everywhere, powering AI applications from chatbots to image search.

But what _is_ a vector database, really? And how does it work?

To answer that, I set out to build my own (very simple) vector database from scratch - and in this article, I’ll walk you through what I discovered along the way.

## My Requirements

Recently I went through Chroma's [Getting Started](https://docs.trychroma.com/docs/overview/getting-started) guide, which is what I'll be trying to recreate.

In a nutshell, I want to be able to perform two tasks:

1. Add a _document_ to the database.
2. Query the database to find the most relevant _document_.

For simplicity, when I say _document_ I really mean a short sentence. Here are the five sentences I’ll be working with:

- "The cat is sleeping on the chair"
- "A river flows through the valley"
- "She loves playing the piano"
- "Paris is the capital city of France"
- "The spacecraft landed safely on the moon"

The goal is simple: store these _documents_, then send in a query (another piece of text) and get back the one that’s most relevant.

But that raises first question: how do you store text inside a database designed for **vectors**? To answer that, we first need to talk about **embeddings**.

## What Are Embeddings?

Let’s start with a simple example. Consider these four words:

- "man"
- "woman"
- "boy"
- "girl"

Imagine we draw a graph where the **x-axis** represents gender and the **y-axis** represents age. We could plot each word as a point on that graph:

![Semantic Space](./images/building-vector-db/semantic-space.png 'Semantic space')

Here, gender and age act as **semantic features** — dimensions that capture part of each word’s meaning. Each feature can be expressed as a number, giving us coordinates for every word:

![Coordinates](./images/building-vector-db/coordinates.png 'Coordinates')

Now, let’s add another word: **"king"**. It shares the same gender and age attributes as "man," but obviously the meanings aren’t identical. To capture this difference, we’d need to add another semantic feature — say, **royalty**. With this third axis, our words now live in a **3D space**.

![3D Space](./images/building-vector-db/3d-space.png '3D space')

Each word can now be described by three numbers: age, gender, and royalty. We call this list of numbers a **vector**. Since the numbers represent semantic features, it’s often called a **feature vector**.

Once words are expressed as vectors, we can compare them mathematically. One way is to measure the Euclidean distance between the points. For example, we see that "boy" is closer to "man" than to "woman".

![Distance](./images/building-vector-db/distance.png 'Distance between points')

Of course, three features aren’t nearly enough to capture the full meaning of language. Where would you plot words like _"cat,"_ _"run,"_ or _"Mars"_? In practice, embedding models use **hundreds or even thousands of dimensions**. For example, OpenAI’s `text-embedding-3-large` model creates embeddings with **3,072 dimensions**.

One last note: in our example, we labeled features like gender, age, and royalty to make things intuitive. Real embeddings don’t have neatly interpretable axes. They’re generated by machine learning models that learn useful patterns automatically — the dimensions themselves are abstract, but together they capture semantic meaning.

With this in mind, we can now turn our sample _documents_ into embeddings.

## Encoding

Now that we understand what embeddings are — vectors that represent the semantic features of words — it’s time to **turn our documents into embeddings** so they can live in a vector database.

This process has a few stages, which we’ll walk through using our first document: _“The cat is sleeping on the couch.”_

### Text preprocessing

Before encoding, we usually clean and preprocess the text. This helps simplify the input for embedding models, although modern models like OpenAI’s or Hugging Face’s can handle raw text directly. Common preprocessing steps include:

- **Tokenization**: splitting the text into words or subwords
- **Lowercasing**
- **Removing punctuation**
- **Stemming**: reducing words to their root form (e.g., _“running” → “run”_)
- **Lemmatization**: mapping words to their base form (e.g., _“better” → “good”_)

In practice, modern embeddings (like OpenAI’s or Hugging Face models) usually skip manual preprocessing and handle raw text. But since we’re building from scratch, we’ll keep things explicit.

For tokenization, I'll be using the [`nltk`](https://www.nltk.org/) (Natural Language Toolkit) library.

For our example, we’ll use [`nltk`](https://www.nltk.org/?utm_source=chatgpt.com) (Natural Language Toolkit) library to tokenize, lowercase, and remove punctuation:

```python
from nltk.tokenize import word_tokenize
import string

document = documents[0] # "The cat is sleeping on the couch."

# Tokenization
words = word_tokenize(document)

# Lowercasing and removing punctuation
preprocessed_words = [word.lower() for word in words if word not in string.punctuation]
```

At this point, our _document_ is a clean list of tokens, ready for embedding.

### Word embeddings

Once we have preprocessed tokens, the next step is to convert each token into a **vector representation**, also called a **word embedding**. There are several ways to obtain these embeddings:

- **Pre-trained embeddings**: like GloVe or FastText, which come ready-made.
- **Contextual embeddings**: from models like BERT or GPT, which take surrounding context into account.
- **Custom-trained embeddings**: train your own embedding model on a dataset of your choice.

I decided to **train a small Word2Vec model from scratch**.

Word2Vec is a technique that turns words into vectors by learning from their contexts in large text corpora. It was developed by Tomas Mikolov and his team at Google in 2013. I won’t go too deep into the inner workings here - it’s beyond the scope of this article, and honestly, I’m still wrapping my head around it myself. But if you want to explore it further, check out these excellent resources:

- [The Illustrated Word2Vec by Jay Alammar](https://jalammar.github.io/illustrated-word2vec/?utm_source=chatgpt.com)
- [Word2Vec Explained on YouTube](https://www.youtube.com/watch?v=viZrOnJclY0&utm_source=chatgpt.com)

To train our Word2Vec model, we need a dataset. Luckily, [Hugging Face](https://huggingface.co/) provides a [collection of Wikipedia articles](https://huggingface.co/datasets/wikimedia/wikipedia) that we can use:

```python
from datasets import load_dataset
from nltk.tokenize import sent_tokenize, word_tokenize
import string

# https://huggingface.co/datasets/wikimedia/wikipedia
dataset = load_dataset("wikimedia/wikipedia", "20231101.en", split="train")

# Preprocess text
all_sentences = []

# Using first 50,000 articles
for article in dataset["text"][:50000]:
    for sent in sent_tokenize(article):
        tokens = [word.lower() for word in word_tokenize(sent.lower()) if word not in string.punctuation]
        if tokens:  # skip empty sentences
            all_sentences.append(tokens)
```

⚠️ **Note:** real embedding models are trained on much larger datasets. Here we’re using 50,000 articles just to make it feasible on my laptop.

Next, we’ll train a Word2Vec model using the [`gensim`](https://pypi.org/project/gensim/) library. We’ll set the embedding size to **100 dimensions**, meaning each word will be represented as a 100-element vector:

```python
import gensim

# Build a model using the CBOW approach
model = gensim.models.Word2Vec(
    sentences=all_sentences,
    vector_size=100,
    window=5,
    min_count=5,   # ignore very rare words
)
```

Once the model is trained, we can retrieve the embedding for each token in our document:

```python
word_embeddings = [model.wv[word] for word in preprocessed_words]
```

This gives us one 100-dimensional vector per token:

![Word Embeddings](./images/building-vector-db/word-embedding.png 'Word-level embeddings')

### Document embeddings

Now we have five word-level embeddings. But we want a **single embedding for the entier _document_**.

These are some of the common strategies for combining word embeddings:

- **Mean pooling**: Calculate the element-wise mean of all the word embeddings
- **Sum pooling**: Calculate the element-wise sum of all the word embeddings
- **Max pooling**: Take the maximum value along each dimension

I decided to do mean pooling:

```python
embedding = np.mean(word_embeddings, axis=0)
```

![Document Embedding](./images/building-vector-db/document-embedding.png 'Document embedding')

We can use the following function to generate embeddings for all our documents:

```python
def embed(document):
    # word tokenization + lowercasing + pucntuation removal
    words = []
    [word.lower() for word in word_tokenize(sent.lower()) if word not in string.punctuation]
    for word in [word.lower() for word in word_tokenize(document.lower()) if word not in string.punctuation]:
        if word:
            words.append(word)

    # get word-level embeddings
    word_embedding = [model.wv[word] for word in words]

    # combine them into a single vector representation
    embedding = np.mean(word_embedding, axis=0)
    return embedding
```

![Embedded documents](./images/building-vector-db/documents-embedding.png 'Embedded documents')

### Storing embeddings

Now we have a vector for each document. In real systems, vectors are usually stored in a vector database, while the raw text is stored separately:

- Vector DB:

```json
{
  "id": "doc_123",
  "vector": [0.12, -0.83, 0.44, ...]
}
```

- Document Store (SQL, NoSQL...):

```json
{
  "id": "doc_123",
  "text": "The capital of France is Paris."
}
```

For simplicity, we’ll store both together in a Python list of dictionaries:

```python
db_list = []

for doc in documents:
    new_db_entry = {
        "vector": embed(doc),
        "content": doc
    }

    db_list.append(new_db_entry)
```

At this point, we have a **mini in-memory vector database** ready for queries.

## Query

Now that we have a list of embedded _documents_, it’s time to tackle our second requirement: **finding the most relevant document for a given query**.

The key idea is simple: we need to compare the query with the stored _document_ embeddings. But before we can do that, we first need to turn the query itself into a vector using the **same preprocessing and embedding process** as we used for the _documents_.

Here’s the workflow for handling a query:

1. Preprocess the query (tokenize, lowercase, remove punctuation).
2. Embed each token into a vector (using the same embedding model).
3. Combine the token embeddings into a single **document-level embedding**.

Once we have the query embedding, it’s in the same “format” as the stored document vectors. We can now start comparing vectors to determine which document is the most relevant.

But that raises the next question: **how do we measure similarity between two vectors?**

## Measuring Similarity

Once we have document embeddings, we need a way to measure **how similar two vectors are**. Let’s look at two common methods: **Euclidean distance** and **Cosine similarity**.

### Euclidean distance

Euclidean distance measures the "straight-line" distance between two vectors in any-dimensional space:

![Euclidean distance](./images/building-vector-db/euclidean-distance.png 'Euclidean distance')

It’s intuitive and based on the Pythagorean theorem:

![Pythagorean theorem](./images/building-vector-db/pythagorean.png 'Pythagorean theorem')

In Python, using `NumPy`, we can calculate it with `linalg.norm()`, which returns the norm (magnitude) of a vector:

```python
def euclidean_distance(vector1, vector2):
    return np.linalg.norm(vector1 - vector2)
```

Euclidean distance works well in low dimensions, but it’s **not the preferred method for word embeddings**, especially in high-dimensional spaces. That’s where cosine similarity comes in.

### Cosine similarity

Cosine similarity measures the **angle between two vectors**, while ignoring their lengths.

Think of it this way:

- **Euclidean distance** asks: _How far apart are these points?_
- **Cosine similarity** asks: _Are these vectors pointing in the same direction?_

This is especially useful in high-dimensional embeddings, where the **direction** of a vector captures meaning more reliably than its length.

The value of cosine similarity is simply **the cosine of the angle** between two vectors $A$ and $B$:

```math
\text{cosine\_similarity}(A,B) = cos(\theta)
```

Where $\theta$ is the angle between the vectors.

How do we calculate $cos(\theta)$? By using the **dot product**:

$$
A\cdot B=|A|\ |B|\ cos(\theta)
$$

Where:

- $A\cdot B$ is the dot product,
- $|A|$ and $|B|$ are the magnitudes (lengths) of the vectors,
- $\theta$ is the angle between them.

Rearranging gives us the formula for cosine similarity:

```math
\text{cosine\_similarity}(A,B) = \frac{A \cdot B}{|A| \, |B|}
```

The result ranges from **-1 to 1**, with higher values indicating more similar vectors. In practice, embeddings usually fall between **0 and 1**, since negative similarity is rare.

Using `NumPy`, we can compute cosine similarity like this:

```python
def cosine_sim(vector1, vector2):
    return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))
```

With NumPy, this is straightforward to compute:

- It requires fewer arithmetic operations than Euclidean distance.
- Vector **magnitude** often carries less meaning than direction.
- In high-dimensional spaces, Euclidean distance suffers from the **curse of dimensionality**, where distances between points tend to become similar, reducing discriminative power.

Cosine similarity, by focusing on direction rather than length, provides a more reliable measure of semantic similarity for embeddings.

### Back to querying

Let’s return to our example. Suppose we want to query for:

- _"domestic pet"_

After embedding the query, we can compare it to the embeddings stored in our database using **Euclidean distance** or **cosine similarity** to find the most relevant document.

```
Query: 'domestic pet'

The cat is sleeping on the chair.:
    Euclidean distance: 8.822952270507812
    Cosine similarity: 0.36733609437942505


A river flows through the valley...:
    Euclidean distance: 12.136998176574707
    Cosine similarity: 0.06201300770044327


She loves playing the piano!:
    Euclidean distance: 11.284441947937012
    Cosine similarity: 0.14509940147399902


Paris is the capital city of France.:
    Euclidean distance: 10.82635498046875
    Cosine similarity: 0.16355328261852264


The spacecraft landed safely on the moon!:
    Euclidean distance: 10.356178283691406
    Cosine similarity: 0.18387603759765625
```

As expected, the document _"The cat is sleeping on the chair"_ has the **lowest Euclidean distance** and the **highest cosine similarity**, making it the closest in meaning to our query.

Here are a few more queries I tried:

<Query client:idle />

This is already looking promising! For any new query, we can embed it, compare it with all stored embeddings, and return the document with the highest cosine similarity. So grab a well-deserved cup of coffee - we’ve just built our own **in-memory vector database**!

For our small dataset, this **brute-force approach** works perfectly. But imagine having thousands - or millions - of documents: comparing every single one would be very inefficient, with a time complexity of **O(n)**.

However, as datasets grow, we need smarter ways to make similarity search more efficient. That’s where **indexing** comes in.

## Indexing

Before we dive in, let’s answer the obvious question: **what is an index?**

An **index** is a data structure that improves the speed of data retrieval operations. In a vector database, it’s a structure that allows us to quickly search through and retrieve **high-dimensional vector embeddings**.

Right now, our database is just a **list**. That means whenever we query it, we must compare the query embedding to _every single stored embedding_. This brute-force approach works fine for a small dataset, but as soon as we scale to thousands or millions of vectors, it becomes painfully slow. Indexes solve this problem by organizing vectors in smarter ways.

### Types of vector indexes

There are several approaches to indexing high-dimensional vectors, each with different trade-offs in **speed**, **accuracy**, and **storage**:

1. **Quantization-based indexes**
   - Reduce the precision of vectors to save memory and speed up search.
   - Example: Product Quantization (PQ).
2. **Graph-based indexes**
   - Represent vectors as nodes in a graph, connecting nearby vectors.
   - Search follows the edges to find neighbors efficiently.
3. **Hash-based indexes**
   - Use **hashing functions** to map similar vectors to the same buckets.
   - Locality-Sensitive Hashing (LSH) is a common example.
4. **Tree-based indexes**
   - Organize vectors in a hierarchical tree structure, partitioning space recursively.
   - Examples include KD-trees and Ball trees.

### Nearest neighbor search

Indexes allow us to quickly answer one key question:

> Given a query vector, which stored vector is the closest?

We can use a tree-based index, like a KD-tree, to find the **exact nearest neighbor** - the vector that is truly closest to our query. KD-trees work well for **low-dimensional data**, but as the number of dimensions increases (hundreds or thousands, as with typical embeddings), their efficiency degrades and ends up being almost as slow as comparing every vector one by one (with time complexity reaching O(n)).

That's why most modern systems rely on **approximate nearest neighbor (ANN) search** instead. Rather than guaranteeing the absolute closest vector, ANN quickly returns vectors that are likely very close to the query. This approach is much faster and scales efficiently for very large or high-dimensional datasets, though it occasionally sacrifices perfect accuracy.

One popular ANN indexing method is [**Hierarchical Navigable Small Worlds (HNSW)**](https://www.pinecone.io/learn/series/faiss/hnsw/), which organizes vectors in a multi-layered graph to efficiently navigate toward the nearest neighbors.

## Summary

Let’s recap the main ideas:

- Vector databases store **embeddings** - high-dimensional vectors that represent the semantic meaning of text, images, or other data.
- Before inserting new data, we first need to convert it into an embedding.
- Similarity between embeddings can be measured in different ways, with **Euclidean distance** and **Cosine similarity** being the most common.
- For efficient querying, embeddings are stored in **indexes** - **data structures** designed to speed up retrieval.
