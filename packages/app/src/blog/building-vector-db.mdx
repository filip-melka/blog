---
banner: ./banners/building-vector-db.gif
description: >-
  I’ve been wondering how vector databases really work, so I decided to try
  building a tiny one myself. Along the way, I learned a bit about embeddings
  and similarity search, and in this article, I’m sharing what I’ve managed to
  figure out so far.
slug: building-vector-db
tags:
  - vector-db
  - embedding
  - ai
title: So… What Even Is a Vector Database?
pubDate: 2025-09-05T15:42:04.584Z
---

import { Query } from '@blog/ui-library'

Vector databases seem to be everywhere these days. Names like [**Pinecone**](https://www.pinecone.io/), [**Weaviate**](https://weaviate.io/), [**Milvus**](https://milvus.io/), and [**Chroma**](https://www.trychroma.com/) keep popping up in blog posts, podcasts, and [Fireship's videos](https://www.youtube.com/watch?v=klTvEwg3oJ4). They’ve quickly become key tools for building AI-powered applications - everything from chatbots to recommendation engines and image search.

But what _is_ a vector database, really? And how does it actually work?

I didn’t know, so I decided to learn the best way I could: by building a tiny, stripped-down version myself. Nothing fancy, just enough to understand the core concepts. In this article, I’ll share what I discovered about embeddings, similarity, and how all these pieces come together in a simple in-memory vector database.

To keep things concrete, I set a small set of requirements inspired by Chroma’s [Getting Started](https://docs.trychroma.com/docs/overview/getting-started?utm_source=chatgpt.com) guide - just simplified for learning purposes.

## Requirements For My Vector Database

In a nutshell, I want to be able to perform two tasks:

1. Add a _document_ to the database.
2. Query the database to find the most relevant _document_.

For simplicity, when I say _document_ I really mean a short sentence. But the concepts we'll be exploring apply to other formats as well. Here are the five sentences I’ll be working with:

- "The cat is sleeping on the chair"
- "A river flows through the valley"
- "She loves playing the piano"
- "Paris is the capital city of France"
- "The spacecraft landed safely on the moon"

```python
documents=[
    "The cat is sleeping on the chair",
    "A river flows through the valley",
    "She loves playing the piano",
    "Paris is the capital city of France",
    "The spacecraft landed safely on the moon"
]
```

The goal is simple: store these _documents_, then send in a query (another piece of text) and get back the one that’s most relevant.

But that raises the first question: how do you store text inside a database designed for **vectors** (hence the name "vector database")? To answer that, we first need to talk about **embeddings**.

## What Are Embeddings?

Let’s start with a simple example. Consider these four words:

- "man"
- "woman"
- "boy"
- "girl"

Imagine we draw a graph where the **x-axis** represents gender and the **y-axis** represents age. We could plot each word as a point on that graph:

![Semantic Space](./images/building-vector-db/semantic-space.png 'Semantic space')

Here, _gender_ and _age_ act as **semantic features** — dimensions that capture part of each word’s meaning. Each feature can be expressed as a number, giving us coordinates for every word:

![Coordinates](./images/building-vector-db/coordinates.png 'Coordinates')

Now, let’s add another word: **"king"**. It shares the same _gender_ and _age_ attributes as "man," but obviously the meanings aren’t identical. To capture this difference, we’d need to add another semantic feature — say, **royalty**. With this third axis, our words now live in a **3D space**.

![3D Space](./images/building-vector-db/3d-space.png '3D space')

Each word can now be described by three numbers: _age_, _gender_, and _royalty_. That list of numbers - a vector - is what we call an **embedding**.

And just so there's no confusion let's answer the following question:

> What's the difference between an embedding and a vector?

A **vector** is just an ordered list of numbers. Nothing more, nothing less. An **embedding** is a vector created to represent something (like a word, sentence, or image) in a way that captures its meaning. So every embedding is a vector, but not every vector is an embedding. However, you'll often find these words used interchangeably (including this article).

The real power of embeddings comes from the fact that we can compare them mathematically. One way is to measure the straight line between the points (also known as Euclidean distance, more on that later). For example, we see that "boy" is closer to "man" than to "woman".

![Distance](./images/building-vector-db/distance.png 'Distance between points')

Of course, three features aren’t nearly enough to capture the full meaning of language. Where would you plot words like _"cat,"_ _"run,"_ or _"Mars"_? In practice, embeddings live in **hundreds or even thousands of dimensions**. For example, OpenAI’s `text-embedding-3-large` model produces embeddings with **3,072 dimensions**.

⚠️ **One last note:** in our example, we labeled features like _gender_, _age_, and _royalty_ to make things intuitive. Real embeddings don’t have neatly interpretable axes. They’re generated by machine learning models that learn useful patterns automatically — the dimensions themselves are abstract, but together they capture semantic meaning.

With this in mind, we can now turn our sample _documents_ into embeddings.

## Encoding

Now that we understand what embeddings are — vectors that represent the semantic features of words — it’s time to **turn our documents into embeddings** so they can live in a vector database.

This process has a few stages, which we’ll walk through using our first document: _“The cat is sleeping on the couch.”_

```python
document = documents[0] # "The cat is sleeping on the couch"
```

### Text preprocessing

Before encoding, we usually clean and preprocess the text. This helps simplify the input for embedding models, although modern models like OpenAI’s or Hugging Face’s can handle raw text directly. Common preprocessing steps include:

- **Tokenization**: splitting the text into words or subwords
- **Lowercasing**
- **Removing punctuation**
- **Stemming**: reducing words to their root form (e.g., _“running” → “run”_)
- **Lemmatization**: mapping words to their base form (e.g., _“better” → “good”_)

For tokenization, I'll be using the [`nltk`](https://www.nltk.org/) (Natural Language Toolkit) library.

```python
from nltk.tokenize import word_tokenize

# Tokenization
words = word_tokenize(document)

# Lowercasing
preprocessed_words = [word.lower() for word in words]
```

At this point, our _document_ is a clean list of tokens (words), ready for embedding.

```
['the', 'cat', 'is', 'sleeping', 'on', 'the', 'chair']
```

### Word embeddings

Once we have preprocessed words (tokens), the next step is to turn each word into a **vector representation**, also called a **word embedding**. There are a few common ways to get these embeddings:

- **Pre-trained embeddings** – Models like GloVe or FastText provide ready-made vectors you can download and use directly. These embeddings are typically static, meaning each word has a single fixed vector, regardless of context.
- **Contextual embeddings** – Modern models like BERT or GPT generate vectors that depend on the word’s surrounding context (so “bank” in “river bank” and “bank account” would have different embeddings).
- **Custom-trained embeddings** – You can also train your own embedding model on a dataset of your choice, which is great if you need a domain-specific vocabulary (e.g., medicine, law).

For my project, I went with the third option: I decided to **train a small Word2Vec model**.

**Word2Vec** is one of the earliest and most influential techniques for creating embeddings. Developed by Tomas Mikolov and his team at Google in 2013, it learns to represent words as vectors by looking at the contexts in which they appear. Depending on your needs, you can either:

- **download a pre-trained Word2Vec model** trained on huge corpora (like Google News), or
- **train your own Word2Vec model** on a custom dataset.

I won’t go too deep into the math or architecture here (it’s still something I’m wrapping my head around), but if you’d like to explore Word2Vec further, check out these fantastic resources:

- [The Illustrated Word2Vec by Jay Alammar](https://jalammar.github.io/illustrated-word2vec/?utm_source=chatgpt.com)
- [Word2Vec Explained on YouTube](https://www.youtube.com/watch?v=viZrOnJclY0&utm_source=chatgpt.com)

To train our _Word2Vec_ model, we need a dataset. Luckily, [Hugging Face](https://huggingface.co/) provides a [collection of Wikipedia articles](https://huggingface.co/datasets/wikimedia/wikipedia) that we can use:

```python
from datasets import load_dataset
from nltk.tokenize import sent_tokenize, word_tokenize

# https://huggingface.co/datasets/wikimedia/wikipedia
dataset = load_dataset("wikimedia/wikipedia", "20231101.en", split="train")

# Training data (individual words from Wikipedia articles)
training_words = []

# Using first 50,000 articles for demonstration
for article in dataset["text"][:50000]:
	for sent in sent_tokenize(article):
		tokens = [word.lower() for word in word_tokenize(sent.lower())]
		if tokens:  # skip empty words
			training_words.append(tokens)
```

⚠️ **Note:** real embedding models are trained on much larger datasets. Here we’re using 50,000 articles just to make it feasible on my laptop.

Next, we’ll train a Word2Vec model using the [`gensim`](https://pypi.org/project/gensim/) library. We’ll set the embedding size to **100 dimensions**, meaning each word will be represented as a 100-element vector:

```python
import gensim

# Build a model using the CBOW approach
model = gensim.models.Word2Vec(
    sentences=training_words,
    vector_size=100,
    window=5,
    min_count=5,   # ignore very rare words
)
```

Once the model is trained, we can retrieve the embedding for each token in our document:

```python
word_embeddings = [model.wv[word] for word in preprocessed_words]
```

This gives us one 100-dimensional vector per token:

![Word Embeddings](./images/building-vector-db/word-embedding.png 'Word-level embeddings')

### Document embeddings

Now we have seven word-level embeddings. But we want a **single embedding for the entire _document_**.

These are some of the common strategies for combining word embeddings:

- **Mean pooling**: Calculate the element-wise mean of all the word embeddings
- **Sum pooling**: Calculate the element-wise sum of all the word embeddings
- **Max pooling**: Take the maximum value along each dimension

I decided to do mean pooling:

```python
docuemtn_embedding = np.mean(word_embeddings, axis=0)
```

![Document Embedding](./images/building-vector-db/document-embedding.png 'Document embedding')

We can use the following function to generate embeddings for all our documents:

```python
from nltk.tokenize import word_tokenize

def embed(document):
    # word tokenization + lowercasing + pucntuation removal
    words = []
    [word.lower() for word in word_tokenize(document.lower())]
    for word in [word.lower() for word in word_tokenize(document.lower())]:
        if word:
            words.append(word)

    # get word-level embeddings
    word_embedding = [model.wv[word] for word in words]

    # combine them into a single vector representation
    docuemtn_embedding = np.mean(word_embedding, axis=0)
    return docuemtn_embedding
```

![Embedded documents](./images/building-vector-db/documents-embedding.png 'Embedded documents')

### Storing embeddings

Now we have an embedding for each document. In real systems, embeddings are usually stored in a vector database, while the raw text is stored separately:

- Vector DB:

```json
{
  "id": "doc_123",
  "vector": [0.12, -0.83, 0.44, ...]
}
```

- Document Store (SQL, NoSQL...):

```json
{
  "id": "doc_123",
  "text": "The cat is sleeping on the chair"
}
```

To keep things simple, we’ll put the embedding (vector) and its content together in a Python dictionary. Each dictionary will then be added to a list.

```python
db_list = []

for doc in documents:
    new_db_entry = {
        "vector": embed(doc),
        "content": doc
    }

    db_list.append(new_db_entry)
```

At this point, we have a **mini in-memory vector database** ready for queries.

## Query

Now that we have a list of embedded _documents_, it’s time to tackle our second requirement: **finding the most relevant document for a given query**.

The key idea is simple: we need to compare the query with the stored _document_ embeddings. But before we can do that, we first need to turn the query itself into an embedding using the **same preprocessing and embedding process** as we used for the _documents_. This is important as different models produce different embeddings.

Just to recap, here’s what happens with a query:

1. Preprocess the query (tokenize, lowercase).
2. Turn each token into an embedding (using our custom-trained _word2vec_ model).
3. Combine the word-level embeddings into a single **query-level embedding**.

Once we have the query embedding, we can start comparing the vectors to determine which document is the most relevant.

But that raises the next question: **how do we measure similarity between two vectors?**

## Measuring Similarity

Once we have document embeddings, we need a way to measure **how similar two embeddings (vectors) are**. Let’s look at two common methods: **Euclidean distance** and **Cosine similarity**.

### Euclidean distance

Euclidean distance measures the "straight-line" distance between two vectors in any-dimensional space:

![Euclidean distance](./images/building-vector-db/euclidean-distance.png 'Euclidean distance')

It’s intuitive and based on the Pythagorean theorem:

![Pythagorean theorem](./images/building-vector-db/pythagorean.png 'Pythagorean theorem')

In Python, using `NumPy`, we can calculate it with `linalg.norm()`, which returns the norm (magnitude) of a vector:

```python
def euclidean_distance(vector1, vector2):
    return np.linalg.norm(vector1 - vector2)
```

Euclidean distance works well in low dimensions, but it’s not the preferred method for document embeddings, especially in high-dimensional spaces. That’s where cosine similarity comes in.

### Cosine similarity

Cosine similarity measures the **angle between two vectors**, while ignoring their lengths.

Think of it this way:

- **Euclidean distance** asks: _How far apart are these points?_
- **Cosine similarity** asks: _Are these vectors pointing in the same direction?_

This is especially useful in high-dimensional embeddings, where the **direction** of a vector captures meaning more reliably than its length.

The value of cosine similarity is simply **the cosine of the angle** between two vectors $A$ and $B$:

```math
\text{cosine\_similarity}(A,B) = cos(\theta)
```

Where $\theta$ is the angle between the vectors.

How do we calculate $cos(\theta)$? By using the **dot product**:

$$
A\cdot B=|A|\ |B|\ cos(\theta)
$$

Where:

- $A\cdot B$ is the [dot product](https://www.mathsisfun.com/algebra/vectors-dot-product.html),
- $|A|$ and $|B|$ are the magnitudes (lengths) of the vectors,
- $\theta$ is the angle between them.

Rearranging gives us the formula for cosine similarity:

```math
\text{cosine\_similarity}(A,B) = \frac{A \cdot B}{|A| \, |B|}
```

The result ranges from **-1 to 1**, with higher values indicating more similar vectors. In practice, embeddings usually fall between **0 and 1**, since negative similarity is rare.

Using `NumPy`, we can compute cosine similarity like this:

```python
def cosine_sim(vector1, vector2):
    return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))
```

These are some of the main reasons why cosine similarity is often preferred over Euclidean distance:

- **Magnitude isn’t as important as direction.** Two embeddings can have very different lengths (norms) but still point in nearly the same direction, which usually means they’re semantically similar.
- **More robust in high dimensions.** Euclidean distance suffers from the _curse of dimensionality_: as dimensions increase, distances between points tend to become indistinguishable. Cosine similarity, by focusing on angles instead of absolute distances, avoids much of this problem.
- **Better semantic alignment.** In practice, embeddings trained for semantic similarity are optimized so that their directions carry more meaning than their magnitudes, making cosine similarity a better fit.

### Back to querying

Let’s return to our example. Suppose we want to query for:

- _"domestic pet"_

After embedding the query, we can compare it to the embeddings stored in our database using **Euclidean distance** or **cosine similarity** to find the most relevant document.

```
Query: 'domestic pet'

The cat is sleeping on the chair.:
    Euclidean distance: 8.822952270507812
    Cosine similarity: 0.36733609437942505


A river flows through the valley...:
    Euclidean distance: 12.136998176574707
    Cosine similarity: 0.06201300770044327


She loves playing the piano!:
    Euclidean distance: 11.284441947937012
    Cosine similarity: 0.14509940147399902


Paris is the capital city of France.:
    Euclidean distance: 10.82635498046875
    Cosine similarity: 0.16355328261852264


The spacecraft landed safely on the moon!:
    Euclidean distance: 10.356178283691406
    Cosine similarity: 0.18387603759765625
```

As expected, the document _"The cat is sleeping on the chair"_ has the **lowest Euclidean distance** and the **highest cosine similarity**, making it the closest in meaning to our query.

Here are a few more queries I tried:

<Query client:idle />

This is already looking promising! For any new query, we can embed it, compare it with all stored embeddings, and return the document with the highest cosine similarity. Congratulations - we’ve just built a tiny, in-memory vector database! 🎉

For our simple example with just a handful of sentences, this **brute-force approach** works fine. But if we had thousands - or even millions - of documents, checking every single one would quickly become impractical. The time complexity is **O(n)**, which doesn’t scale well.

To handle larger datasets, we’ll need something smarter: a way to organize embeddings so that we can find the nearest ones much faster. That’s where **indexing** comes in.

## Indexing

Before we dive in, let’s answer the obvious question: **what is an index?**

An **index** is a data structure that improves the speed of data retrieval operations. In a vector database, it’s a structure that allows us to quickly search through and retrieve **high-dimensional vector embeddings**.

Right now, our "database" is just a **list**. That means whenever we query it, we must compare the query embedding to _every single stored embedding_. This brute-force approach works fine for a small dataset, but as soon as we scale to thousands or millions of vectors, it becomes painfully slow. Indexes solve this problem by organizing vectors in smarter ways.

### Types of vector indexes

There are several approaches to indexing high-dimensional vectors, each with different trade-offs in **speed**, **accuracy**, and **storage**:

1. **Quantization-based indexes**
   - Reduce the precision of vectors to save memory and speed up search.
   - Example: [Product Quantization](https://www.pinecone.io/learn/series/faiss/product-quantization/) (PQ).
1. **Graph-based indexes**
   - Represent vectors as nodes in a graph, connecting nearby vectors.
   - Search follows the edges to find neighbors efficiently.
1. **Hash-based indexes**
   - Use **hashing functions** to map similar vectors to the same buckets.
   - [Locality-Sensitive Hashing](https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/) (LSH) is a common example.
1. **Tree-based indexes**
   - Organize vectors in a hierarchical tree structure, partitioning space recursively.
   - Example: [KD-trees](https://medium.com/@katyayanivemula90/what-is-a-k-dimensional-tree-8265cc737d77).

### Nearest neighbor search

At the core of all this is a simple question:

> Given a query vector, which stored vector is closest?

Tree-based indexes like KD-trees can answer this exactly — they return the **true nearest neighbor**. That works well for low-dimensional data, but as dimensions climb into the hundreds or thousands (as they do with embeddings), KD-trees lose efficiency. In fact, they eventually become almost as slow as brute force, with time complexity approaching **O(n)**.

That’s why modern vector databases usually turn to **approximate nearest neighbor (ANN) search**. Instead of guaranteeing the single closest vector, ANN quickly finds vectors that are _very likely_ close enough. This trade-off makes it dramatically faster and much more scalable, even if it sometimes sacrifices perfect accuracy.

One widely used method here is [**Hierarchical Navigable Small Worlds (HNSW)**](https://www.pinecone.io/learn/series/faiss/hnsw/?utm_source=chatgpt.com), which organizes vectors into a layered graph structure. Searches start broad and then “zoom in,” navigating efficiently toward the nearest neighbors.

We won’t dive deeper into ANN or indexes here, but hopefully this gives you an idea of how vector databases keep similarity search fast even as the data grows huge.

## Summary

Let’s recap the main ideas:

- Vector databases store **embeddings** - high-dimensional vectors that represent the semantic meaning of text, images, or other data.
- Before inserting new data, we first need to convert it into an embedding.
- Similarity between embeddings can be measured in different ways, with **Euclidean distance** and **Cosine similarity** being the most common.
- For efficient querying, embeddings are stored in **indexes** - **data structures** designed to speed up retrieval.
